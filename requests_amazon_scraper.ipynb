{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\fadia\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from time import time\n",
    "from gdoctableapppy import gdoctableapp\n",
    "import os\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from gdoctableapppy import gdoctableapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "URL = \" https://www.amazon.com/Arm-Hammer-Booster-Purifying-Waters/dp/B08T7X7XYX?pd_rd_w=UPYP1&content-id=amzn1.sym.80bfd2e5-c81d-48d5-ae85-c500e2214af9&pf_rd_p=80bfd2e5-c81d-48d5-ae85-c500e2214af9&pf_rd_r=P3BJ0XMXPK4K68KFG564&pd_rd_wg=a6oPV&pd_rd_r=9a304aff-d87b-4135-b186-63dca4d76e6c&pd_rd_i=B08T7X7XYX&psc=1&ref_=pd_bap_d_csi_pd_ys_c_rfy_rp_crs_6_t\"\n",
    "HEADERS = {\n",
    "    \"method\": \"GET\",\n",
    "\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\",\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "#print(response.content)\n",
    "# with open(\"myfile.html\", \"w\") as file1:\n",
    "#     # Writing data to a file\n",
    "#     file1.write(str(response.content))\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "see_all_reviews_link = soup.find(\"a\", {\"data-hook\": \"see-all-reviews-link-foot\"})[\"href\"]\n",
    "\n",
    "\n",
    "URL_ALL_REVIEWS = \"https://www.amazon.com/\" + see_all_reviews_link\n",
    "\n",
    "\n",
    "response = requests.get(URL_ALL_REVIEWS, headers=HEADERS)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Find the element that contains the ratings with reviews and extract the text\n",
    "ratings_with_reviews_elem = soup.find(\"div\", {\"id\": \"filter-info-section\"})\n",
    "ratings_with_reviews_text = ratings_with_reviews_elem.get_text().strip()\n",
    "\n",
    "#get just the reviews amount\n",
    "reviews_count = re.search(r'(\\d+,*\\d*)\\s+with reviews', ratings_with_reviews_text).group(1)\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"\n",
    "    This function scrapes Amazon reviews from a given page URL.\n",
    "\n",
    "    Args:\n",
    "    - page_url: a string representing the URL of the page to be scraped.\n",
    "\n",
    "    Returns:\n",
    "    - page_data: a list of lists containing the scraped data. Each inner list contains 4 elements: the name of the reviewer,\n",
    "    the title of the review, the body of the review, and the rating (number of stars) given by the reviewer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Send a GET request to the URL and retrieve the HTML content of the page\n",
    "    response = requests.get(page_url, headers=HEADERS)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the review divs on the page\n",
    "    reviews = soup.find_all('div', {'data-hook': 'review'})\n",
    "\n",
    "    # Initialize an empty list to store the data scraped from this page\n",
    "    page_data = []\n",
    "\n",
    "    # Loop through each review div and extract the relevant information\n",
    "    for review in reviews:\n",
    "        # Extract the reviewer's name\n",
    "        profile_name = review.find(\"span\", {\"class\": \"a-profile-name\"}).get_text()\n",
    "\n",
    "        # Extract the review title (if it exists)\n",
    "        title_elem = review.find(\"a\", {\"data-hook\": \"review-title\"})\n",
    "        title = title_elem.get_text().strip() if title_elem else \"\"\n",
    "\n",
    "        # Extract the review body\n",
    "        body = review.find(\"span\", {\"data-hook\": \"review-body\"}).get_text().strip()\n",
    "\n",
    "        # Extract the rating (number of stars) given by the reviewer\n",
    "        stars_elem = review.find(\"i\", {\"data-hook\": \"review-star-rating\"})\n",
    "        stars = float(stars_elem.span.get_text().split()[0]) if stars_elem and stars_elem.span else None\n",
    "        if stars==None:\n",
    "            stars_elem = review.find(\"i\", {\"data-hook\": \"cmps-review-star-rating\"})\n",
    "            stars = float(stars_elem.span.get_text().split()[0]) if stars_elem and stars_elem.span else None\n",
    "\n",
    "        # Append the extracted information as a list to the page_data list\n",
    "        page_data.append([profile_name, title, body, stars])\n",
    "\n",
    "    # Return the page_data list containing the scraped information\n",
    "    return page_data\n",
    "\n",
    "# Create a ThreadPoolExecutor object to concurrently execute the scrape_reviews function on multiple pages\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Calculate the number of pages to scrape based on the total number of reviews for the product\n",
    "    page_num=math.ceil(float(reviews_count.replace(\",\",\"\"))/10)\n",
    "\n",
    "    # Generate a list of page URLs to scrape\n",
    "    pages = [f\"{URL_ALL_REVIEWS}+&pageNumber={i+1}\" for i in range(page_num)]\n",
    "\n",
    "    # Use the ThreadPoolExecutor to concurrently scrape the reviews from all the pages in the pages list\n",
    "    results = executor.map(scrape_reviews, pages)\n",
    "\n",
    "    # Combine the results from all the pages into a single list\n",
    "    for result in results:\n",
    "        data += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parker S.</td>\n",
       "      <td>Cleaned my walls with this</td>\n",
       "      <td>I love this mixed in my mop bucket with warm w...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karol Aguilar</td>\n",
       "      <td>good</td>\n",
       "      <td>good alternative to other brands, but the scen...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANIMALGIRL81</td>\n",
       "      <td>They smell so good.</td>\n",
       "      <td>Definitely make my laundry smell great! Will n...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Melissa Y Wood</td>\n",
       "      <td>Value</td>\n",
       "      <td>Smells great and decent price.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foxylady</td>\n",
       "      <td>Terrific product to drive away bad smells</td>\n",
       "      <td>Great little laundry deodorant. Just enough to...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name                                      Title  \\\n",
       "0       Parker S.                 Cleaned my walls with this   \n",
       "1   Karol Aguilar                                       good   \n",
       "2    ANIMALGIRL81                        They smell so good.   \n",
       "3  Melissa Y Wood                                      Value   \n",
       "4        Foxylady  Terrific product to drive away bad smells   \n",
       "\n",
       "                                                Body  Stars  \n",
       "0  I love this mixed in my mop bucket with warm w...    5.0  \n",
       "1  good alternative to other brands, but the scen...    5.0  \n",
       "2  Definitely make my laundry smell great! Will n...    5.0  \n",
       "3                     Smells great and decent price.    5.0  \n",
       "4  Great little laundry deodorant. Just enough to...    5.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=['Name', 'Title', 'Body', 'Stars'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.23202395439148\n"
     ]
    }
   ],
   "source": [
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OpenAI language model with desired temperature\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Instantiate a CharacterTextSplitter object to split text into chunks\n",
    "text_splitter = CharacterTextSplitter()\n",
    "\n",
    "# Define a function to generate summary for a given text\n",
    "def generate_summary(all_text) -> str:\n",
    "    \"\"\"\n",
    "    Generate summary for a given text by first splitting the text into smaller chunks, creating a Document object\n",
    "    for each chunk, and then using a summarize chain to generate summary for each chunk. The function returns the\n",
    "    combined summary for all the chunks.\n",
    "\n",
    "    Parameters:\n",
    "    all_text (str): The text for which summary is to be generated.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated summary for the text.\n",
    "    \"\"\"\n",
    "    # Split the text into chunks using the CharacterTextSplitter object\n",
    "    texts = text_splitter.split_text(all_text)\n",
    "\n",
    "    # Create a Document object for each chunk\n",
    "    docs = [Document(page_content=t) for t in texts[:3]]\n",
    "\n",
    "    # Load a summarize chain using the OpenAI language model\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "    # Generate summary for each chunk\n",
    "    summary=chain.run(docs)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Define a function to generate summary for a list of text chunks\n",
    "def chunks_summary(text_chunks):\n",
    "    \"\"\"\n",
    "    Generate summary for a list of text chunks by first splitting the chunks into smaller chunks with a maximum length\n",
    "    of 15,000 characters, generating summary for each smaller chunk, and then combining the summaries to generate a\n",
    "    final summary for the entire list of text chunks.\n",
    "\n",
    "    Parameters:\n",
    "    text_chunks (list): A list of text chunks for which summary is to be generated.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated summary for the entire list of text chunks.\n",
    "    \"\"\"\n",
    "    # Split the chunks into smaller chunks with a maximum length of 15,000 characters\n",
    "    text_chunks=chunk_data(text_chunks,15000)\n",
    "\n",
    "    # Generate summary for each smaller chunk\n",
    "    summaries = []\n",
    "    for chunk in text_chunks:\n",
    "        summary = generate_summary(chunk)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Combine the summaries to generate the final summary for the entire list of text chunks\n",
    "    if len(summaries)!=1:\n",
    "        return chunks_summary(summaries)\n",
    "    else:\n",
    "        return summaries[0]\n",
    "\n",
    "# Define a function to split data into chunks with a maximum length\n",
    "def chunk_data(data, max_len) -> list: \n",
    "    \"\"\"\n",
    "    Split a list of data into chunks with a maximum length.\n",
    "\n",
    "    Parameters:\n",
    "    data (list): The list of data to be split into chunks.\n",
    "    max_len (int): The maximum length of each chunk.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of chunks.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    c_chuck = \"\"\n",
    "\n",
    "    # Split each element of the list into smaller chunks with a maximum length of 'max_len'\n",
    "    for review in data:\n",
    "        if len(c_chuck+review)>max_len:\n",
    "            if c_chuck:\n",
    "                res.append(c_chuck)\n",
    "                c_chuck = \"\"\n",
    "        if len(review) > max_len:\n",
    "            while len(review) > max_len:\n",
    "                res.append(review[:max_len])\n",
    "                review = review[max_len:]\n",
    "            res.append(review)\n",
    "            c_chuck = \"\"\n",
    "        else:\n",
    "            c_chuck+=review\n",
    "\n",
    "    # Append any remaining text to the list of chunks\n",
    "    if c_chuck:\n",
    "        res.append(c_chuck)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# Generate summary for a list of text chunks\n",
    "summary_of_product=chunks_summary(''.join(df['Body'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Church & Hammer Clean Scentsations scent booster is an affordable and effective product that eliminates odors and provides a long-lasting, pleasant scent. Customers have been pleased with the product and will continue to purchase it.\n"
     ]
    }
   ],
   "source": [
    "print(summary_of_product[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parker S.</td>\n",
       "      <td>Cleaned my walls with this</td>\n",
       "      <td>I love this mixed in my mop bucket with warm w...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karol Aguilar</td>\n",
       "      <td>good</td>\n",
       "      <td>good alternative to other brands, but the scen...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANIMALGIRL81</td>\n",
       "      <td>They smell so good.</td>\n",
       "      <td>Definitely make my laundry smell great! Will n...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Melissa Y Wood</td>\n",
       "      <td>Value</td>\n",
       "      <td>Smells great and decent price.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foxylady</td>\n",
       "      <td>Terrific product to drive away bad smells</td>\n",
       "      <td>Great little laundry deodorant. Just enough to...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name                                      Title  \\\n",
       "0       Parker S.                 Cleaned my walls with this   \n",
       "1   Karol Aguilar                                       good   \n",
       "2    ANIMALGIRL81                        They smell so good.   \n",
       "3  Melissa Y Wood                                      Value   \n",
       "4        Foxylady  Terrific product to drive away bad smells   \n",
       "\n",
       "                                                Body  Stars  \n",
       "0  I love this mixed in my mop bucket with warm w...    5.0  \n",
       "1  good alternative to other brands, but the scen...    5.0  \n",
       "2  Definitely make my laundry smell great! Will n...    5.0  \n",
       "3                     Smells great and decent price.    5.0  \n",
       "4  Great little laundry deodorant. Just enough to...    5.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file','https://www.googleapis.com/auth/drive']\n",
    "creds = None\n",
    "# The file token.json stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists('token.json'):\n",
    "    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.json', 'w') as token:\n",
    "        token.write(creds.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_service = build('drive', 'v3', credentials=creds)\n",
    "file_metadata = {\n",
    "    'name': 'My Document',\n",
    "    'parents': ['1lBmUcDPDAiMqyHt3lBIZSvpOkPN5L8Hc'],\n",
    "    'mimeType': 'application/vnd.google-apps.document'\n",
    "}\n",
    "document = docs_service.files().create(body=file_metadata).execute()\n",
    "document_id=document['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Google Docs API client\n",
    "docs_service = build('docs', 'v1', credentials=creds)\n",
    "\n",
    "\n",
    "# Create the table\n",
    "requests = [{\n",
    "    'insertTable': {\n",
    "        'rows': 1,\n",
    "        'columns':4,\n",
    "        'endOfSegmentLocation': {}\n",
    "    }\n",
    "}]\n",
    "\n",
    "try:\n",
    "    result = docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests}).execute()\n",
    "except HttpError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data=[]\n",
    "table_data.append(df.columns.tolist())\n",
    "# loop over the rows\n",
    "for _, row in df.iterrows():\n",
    "    table_data.append([row['Name'], row['Title'], row['Body'],row['Stars']])\n",
    "\n",
    "table_data2 = [i[::-1] for i in table_data]\n",
    "\n",
    "resource = {\n",
    "    \"oauth2\": creds,\n",
    "    \"documentId\": document_id,\n",
    "    \"tableIndex\": 0,\n",
    "    \"values\": table_data2\n",
    "}\n",
    "\n",
    "res = gdoctableapp.SetValues(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://drive.google.com/file/d/1MhSm56enhZuyHdl0F-SSMoVON_bVGKy_6MGXrlyYpbU/view?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Set up permission metadata\n",
    "permission_metadata = {\n",
    "    'role': 'reader',\n",
    "    'type': 'anyone',\n",
    "}\n",
    "\n",
    "# Create the permission\n",
    "permission = drive_service.permissions().create(\n",
    "    fileId=document_id,\n",
    "    body=permission_metadata,\n",
    "    fields='id'\n",
    ").execute()\n",
    "\n",
    "# Get the permission ID\n",
    "permission_id = permission['id']\n",
    "\n",
    "# Generate the link\n",
    "link = f'https://drive.google.com/file/d/{document_id}/view?usp=sharing'\n",
    "print(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "141b6056e70a629fcfc37c76d11309ae0ce23d1be763896081c7bf037e6ba981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
